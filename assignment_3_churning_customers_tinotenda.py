# -*- coding: utf-8 -*-
"""Assignment_3_Churning_Customers_Tinotenda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10y_YQY0ziPvKsoUfQM0RODpky2C-figM

Importing important library to be used
"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

"""Extracting he relevant features that can define a customer churn"""

CustomerData=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Assignment3/CustomerChurn_dataset.csv')

"""2. Using EDA(Exploratory Data Analysis) skills to find out which customer profiles relate to churning a lot


"""

# Displaying the first few rows of the dataset
CustomerData.head()

#Checking if there are some missing values
CustomerData.isna().head()

CustomerData.isnull().sum()

#Checking Data types
CustomerData.dtypes

CustomerData.shape

# Getting the summary statistics of numerical features
CustomerData.describe()

# Checking the data types of the columns
CustomerData.info()

# Perform one-hot encoding
encodedData = pd.get_dummies(CustomerData)
encodedData

# Combine Churn_No and Churn_Yes columns into a single Churn column
encodedData['Churn'] = encodedData['Churn_Yes']

# Drop Churn_No and Churn_Yes columns
encodedData = encodedData.drop(columns=['Churn_No', 'Churn_Yes'])

encodedData

# Count the number of churned and non-churned customers
churn_counts = encodedData['Churn'].value_counts()

# Create a bar plot of churn distribution
plt.figure(figsize=(6, 4))
plt.bar(churn_counts.index, churn_counts.values)
plt.xlabel('Churn')
plt.ylabel('Count')
plt.title('Churn Distribution')
plt.show()

from scipy.stats import spearmanr

# Assuming you have loaded the data into the 'CustomerData' DataFrame
# and 'Churn' is the target variable name

# Calculate Spearman's rank correlation coefficients between 'Churn' and other variables
rank_correlation_coefficients = encodedData.apply(lambda x: spearmanr(x, encodedData['Churn']).correlation).drop('Churn')

# Sort the rank correlation coefficients in descending order
sorted_correlation_coefficients = rank_correlation_coefficients.sort_values(ascending=False)

# Print the sorted rank correlation coefficients
print(sorted_correlation_coefficients)

import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2

# Split the data into features (X) and target variable (y)
X = encodedData.drop('Churn', axis=1)
y = encodedData['Churn']

# Perform feature selection using chi-squared test
k = 7  # Number of top features to select
selector = SelectKBest(score_func=chi2, k=k)
X_new = selector.fit_transform(X, y)

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_features = X.columns[selected_feature_indices]

# Print the names of selected features
print(selected_features)

encodedData.columns

import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Split the data into features (X) and target variable (y)
X = encodedData.drop('Churn', axis=1)
y = encodedData['Churn']

# Create a Random Forest classifier
rf_model = RandomForestClassifier()

# Train the model
rf_model.fit(X, y)

# Get feature importances
importances = rf_model.feature_importances_

# Create a DataFrame of feature importances
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': importances})

# Sort the DataFrame by importance in descending order
feature_importances = feature_importances.sort_values('Importance', ascending=False)

# Print the sorted feature importances
print(feature_importances)

import pandas as pd
from sklearn.feature_selection import SelectKBest, chi2

# Split the data into features (X) and target variable (y)
X = encodedData.drop('Churn', axis=1)
y = encodedData['Churn']

# Perform feature selection using chi-squared test
k = 12  # Number of top features to select
selector = SelectKBest(score_func=chi2, k=k)
X_new = selector.fit_transform(X, y)

# Get the indices of the selected features
selected_feature_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_features = X.columns[selected_feature_indices]

# Print the names of selected features
print(selected_features)

# Splitting the data into features (X) and target variable (y)
X = encodedData[['tenure', 'MonthlyCharges', 'InternetService_Fiber optic',
       'OnlineSecurity_No', 'DeviceProtection_No internet service',
       'TechSupport_No', 'TechSupport_No internet service',
       'StreamingTV_No internet service',
       'StreamingMovies_No internet service', 'Contract_Month-to-month',
       'Contract_Two year', 'PaymentMethod_Electronic check']]
y = encodedData['Churn']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

input_shape = (12,)
num_classes = 2

inputs = Input(shape=input_shape)

hidden1 = Dense(64, activation='relu')(inputs)
hidden2 = Dense(32, activation='relu')(hidden1)
hidden3 = Dense(64, activation='relu')(hidden2)
hidden4 = Dense(32, activation='relu')(hidden3)
hidden5 = Dense(64, activation='relu')(hidden4)
hidden6 = Dense(32, activation='relu')(hidden5)
hidden7 = Dense(32, activation='relu')(hidden6)
hidden8 = Dense(64, activation='relu')(hidden7)

outputs = Dense(num_classes, activation='softmax')(hidden8)

model = Model(inputs=inputs, outputs=outputs)

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, batch_size=32, epochs=80, validation_data=(X_test, y_test))

y_pred = model.predict(X_test)

import numpy as np
y_pred_labels = np.argmax(y_pred, axis=1)

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred_labels)
print("Accuracy:", accuracy)

from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(y_test, y_pred[:, 1])
print("AUC Score:", auc_score)

model.save('/content/drive/My Drive/Colab Notebooks/Assignment3/TrainedModel')